import pandas as pd
import torch
import numpy as np
from model import KronosTokenizer

TOKENIZER_PATH = "NeoQuasar/Kronos-Tokenizer-base"
TRAIN_DATA = "synthetic_data/train.npy"

def main():
    print("ğŸ” æ­£åœ¨æ£€æŸ¥ Tokenizer å¯¹åˆæˆæ•°æ®çš„å¤„ç† (ä¿®å¤ç‰ˆ)...")
    tokenizer = KronosTokenizer.from_pretrained(TOKENIZER_PATH)
    
    # åŠ è½½ä¸€æ¡æ•°æ®
    data = np.load(TRAIN_DATA, allow_pickle=True)
    sample = data[0]
    # è·å– numpy array (60, 5)
    values = sample['df'] 
    
    print(f"\n--- åŸå§‹æ•°æ®å½¢çŠ¶: {values.shape} ---")
    print(values[:5]) # æ‰“å°å‰5è¡Œçœ‹çœ‹æ•°å€¼
    
    # --- å…³é”®ä¿®å¤ï¼šæ‰‹åŠ¨è½¬ Tensor ---
    # 1. è½¬ä¸º Tensor
    # 2. è½¬ä¸º float32 (ç¥ç»ç½‘ç»œåªåƒ float32)
    # 3. å¢åŠ  batch ç»´åº¦: [60, 5] -> [1, 60, 5]
    input_tensor = torch.tensor(values, dtype=torch.float32).unsqueeze(0)
    
    print(f"\n--- è¾“å…¥ Tensor å½¢çŠ¶: {input_tensor.shape} ---")
    
    # Tokenize
    try:
        encoded = tokenizer.encode(input_tensor)
        
        if isinstance(encoded, (tuple, list)) and len(encoded) == 2:
            s1, s2 = encoded[0], encoded[1]
        else:
            s1 = encoded
            
        # ç§»é™¤ batch ç»´åº¦æ–¹ä¾¿æŸ¥çœ‹
        s1 = torch.tensor(s1).squeeze()
        
        print("\n--- Tokenizer è¾“å‡º (S1 IDs) ---")
        print(s1)
        
        # ç»Ÿè®¡å”¯ä¸€å€¼
        unique_tokens = torch.unique(s1)
        print(f"\nğŸ“Š å”¯ä¸€ Token æ•°é‡: {len(unique_tokens)}")
        
        if len(unique_tokens) < 5:
            print("âŒ Tokenizer è¾“å‡ºå¤ªå•ä¸€ï¼Œåˆæˆæ•°æ®å¯èƒ½è¿˜æ˜¯æœ‰é—®é¢˜ã€‚")
        else:
            print("âœ… Tokenizer å·¥ä½œæ­£å¸¸ï¼ç¡®å®è¾“å‡ºäº†å¤šæ ·åŒ–çš„ IDã€‚")
            print("ğŸ‰ ç»“è®ºï¼šä¹‹å‰çš„è®­ç»ƒå¤±è´¥æ˜¯å› ä¸ºè¾“å…¥æ ¼å¼ä¸å¯¹ï¼Œæ¨¡å‹ä¸€ç›´åœ¨åƒâ€˜0â€™ã€‚")
            
    except Exception as e:
        print(f"âŒ ä¾ç„¶æŠ¥é”™: {e}")

if __name__ == "__main__":
    main()
