import os
import pandas as pd
import torch
from torch.utils.data import Dataset
from datetime import datetime

class QuantLabelerDataset(Dataset):
    def __init__(self, data_dir, label_dir, tokenizer, seq_len=60):
        """
        å‚æ•°:
        - data_dir: åŸå§‹è¡Œæƒ…æ–‡ä»¶å¤¹è·¯å¾„ (ä½ çš„ 'data/')
        - label_dir: æ ‡æ³¨æ–‡ä»¶æ–‡ä»¶å¤¹è·¯å¾„ (ä½ çš„ 'labels/')
        - tokenizer: Kronos çš„åˆ†è¯å™¨
        - seq_len: åºåˆ—é•¿åº¦ (é»˜è®¤ 60)
        """
        self.tokenizer = tokenizer
        self.seq_len = seq_len
        self.samples = [] # å­˜å‚¨æ‰€æœ‰å‡†å¤‡å¥½çš„æ ·æœ¬ç´¢å¼•

        # 1. æ‰«ææ‰€æœ‰æ ‡æ³¨æ–‡ä»¶
        label_files = [f for f in os.listdir(label_dir) if f.endswith("_labels.csv")]

        print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ•°æ®... å‘ç° {len(label_files)} ä¸ªæ ‡æ³¨æ–‡ä»¶")

        for l_file in label_files:
            # è§£æå“ç§å (ä¾‹å¦‚ "rb_labels.csv" -> "rb.csv")
            symbol_key = l_file.replace("_labels.csv", "")
            raw_file = f"{symbol_key}.csv"
            raw_path = os.path.join(data_dir, raw_file)
            label_path = os.path.join(label_dir, l_file)

            # å¿…é¡»åŒæ—¶å­˜åœ¨åŸå§‹æ•°æ®å’Œæ ‡æ³¨æ•°æ®
            if not os.path.exists(raw_path):
                continue

            # 2. åŠ è½½æ•°æ®
            df_raw = pd.read_csv(raw_path)
            df_label = pd.read_csv(label_path)

            # ç»Ÿä¸€æ—¶é—´æ ¼å¼ (éå¸¸å…³é”®ï¼Œé˜²æ­¢å­—ç¬¦ä¸²å’Œdatetimeä¸åŒ¹é…)
            # å‡è®¾ä½ çš„ csv é‡Œæ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼Œæˆ‘ä»¬ç»Ÿä¸€è½¬ä¸º datetime å¯¹è±¡ä»¥ä¾¿æ¯”è¾ƒ
            df_raw['datetime'] = pd.to_datetime(df_raw['datetime'])
            df_label['datetime'] = pd.to_datetime(df_label['datetime'])

            # 3. åŒ¹é…é€»è¾‘ (æŠŠæ ‡æ³¨ç‚¹æ˜ å°„å›åŸå§‹ K çº¿)
            # æˆ‘ä»¬éå†æ¯ä¸€ä¸ªæ ‡æ³¨ç‚¹
            for _, row in df_label.iterrows():
                target_time = row['datetime']
                label = int(row['label'])

                # åœ¨åŸå§‹æ•°æ®ä¸­æ‰¾åˆ°è¿™ä¸€è¡Œ
                # ä½¿ç”¨ searchsorted æˆ–ç›´æ¥ mask æŸ¥æ‰¾ (æ•°æ®é‡ä¸å¤§ç›´æ¥ mask å³å¯)
                # æ‰¾åˆ° ç›®æ ‡æ—¶é—´ å¯¹åº”çš„ç´¢å¼•
                match = df_raw.index[df_raw['datetime'] == target_time].tolist()
                
                if not match:
                    continue # æ ‡æ³¨çš„æ—¶é—´ç‚¹åœ¨åŸå§‹æ•°æ®é‡Œæ‰¾ä¸åˆ°ï¼ˆå¯èƒ½é‡æ–°ä¸‹è½½äº†æ•°æ®å¯¼è‡´ä¸åŒ¹é…ï¼‰
                
                idx = match[0]

                # 4. æˆªå–å‰ 60 æ ¹ (Sequence)
                # å¦‚æœå‰é¢çš„æ•°æ®ä¸å¤Ÿ 60 æ ¹ï¼Œå°±è·³è¿‡
                if idx < seq_len:
                    continue
                
                # æˆªå–èŒƒå›´: [idx - seq_len : idx] 
                # æ³¨æ„ï¼šæ˜¯å¦åŒ…å«å½“å‰è¿™æ ¹ K çº¿ï¼Ÿé€šå¸¸æ„å›¾è¯†åˆ«æ˜¯åŒ…å«å½“å‰ K çº¿çš„
                # è¿™é‡Œçš„åˆ‡ç‰‡æ˜¯ df_raw.iloc[idx - 59 : idx + 1]
                kline_segment = df_raw.iloc[idx - seq_len + 1 : idx + 1].copy()

                # å­˜å…¥å†…å­˜åˆ—è¡¨
                self.samples.append({
                    'df': kline_segment, # è¿™é‡Œå­˜ DataFrameï¼Œå–çš„æ—¶å€™å† Tokenizeï¼Œçœå†…å­˜
                    'label': label,
                    'info': f"{symbol_key} @ {target_time}" # æ–¹ä¾¿è°ƒè¯•
                })

        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼å…±æ„å»º {len(self.samples)} ä¸ªæœ‰æ•ˆæ ·æœ¬ã€‚")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        item = self.samples[idx]
        df = item['df']
        label = item['label']

        # --- è°ƒç”¨ Kronos Tokenizer ---
        # å‡è®¾ tokenizer æ¥å— pandas dataframe
        # å¦‚æœ tokenizer éœ€è¦ç‰¹å®šåˆ—å (open, high, low, close, volume)ï¼Œè¯·ç¡®ä¿ df é‡Œæœ‰
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ® shiyu-coder çš„å®˜æ–¹ tokenizer API è°ƒæ•´
        try:
            # å‡è®¾ API æ˜¯è¿™æ ·çš„
            input_ids = self.tokenizer.encode(df) 
        except:
            # è¿™æ˜¯ä¸€ä¸ªå ä½ç¬¦ï¼Œé˜²æ­¢ä½ è¿˜æ²¡ä¸‹è½½ tokenizer ä»£ç æŠ¥é”™
            # å®é™…è·‘çš„æ—¶å€™ï¼Œtokenizer.encode ä¼šè¿”å› list æˆ– tensor
            input_ids = [0] * 60 

        # è½¬ Tensor
        if not isinstance(input_ids, torch.Tensor):
            input_ids = torch.tensor(input_ids, dtype=torch.long)
        
        # ç¡®ä¿ç»´åº¦åŒ¹é… (Squeeze/Unsqueeze æ ¹æ®æ¨¡å‹è¦æ±‚)
        # é€šå¸¸ Dataset è¿”å› (Seq_Len,)ï¼ŒDataLoader ä¼šè‡ªåŠ¨å˜æˆ (Batch, Seq_Len)
        return input_ids.squeeze(), torch.tensor(label, dtype=torch.long)

# --- æµ‹è¯•ä»£ç  ---
if __name__ == "__main__":
    # æ¨¡æ‹Ÿæµ‹è¯•
    print("æµ‹è¯• Adapter...")
    # å‡è®¾ä½ å·²ç»æœ‰äº† tokenizer
    class MockTokenizer:
        def encode(self, df): return [1] * 60
    
    dataset = QuantLabelerDataset(
        data_dir="data", 
        label_dir="labels", 
        tokenizer=MockTokenizer()
    )
    
    if len(dataset) > 0:
        x, y = dataset[0]
        print(f"æ ·æœ¬ 0 è¾“å…¥å½¢çŠ¶: {x.shape}, æ ‡ç­¾: {y}")
