import os
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import shutil
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from model import Kronos, KronosTokenizer

# ================= âš™ï¸ é…ç½®åŒºåŸŸ (å·²ä¿®æ”¹) =================
TARGET_FILE = "data/doupo.csv"  # ä½ çš„ç›®æ ‡æ–‡ä»¶
TOKENIZER_PATH = "NeoQuasar/Kronos-Tokenizer-base"
MODEL_PATH = "NeoQuasar/Kronos-small"
WEIGHTS_PATH = "silly_money_head.pth"

SEQ_LEN = 60
STRIDE = 1                 # <--- ä¿®æ”¹ç‚¹ï¼šæ¯æ ¹Kçº¿éƒ½ç®—ï¼Œä¸è·³è¿‡
CONFIDENCE_THRESHOLD = 0.55 # <--- ä¿®æ”¹ç‚¹ï¼šé˜ˆå€¼å¤§å¹…é™ä½ï¼Œæ•æ‰æ›´å¤šä¿¡å·
BATCH_SIZE = 32
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ================= ğŸ—ï¸ æ¨¡å‹å®šä¹‰ =================
class KronosClassifier(nn.Module):
    def __init__(self, model_path):
        super().__init__()
        print(f"æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_path} ...")
        self.backbone = Kronos.from_pretrained(model_path)
        
        # å†»ç»“å‚æ•°
        for param in self.backbone.parameters():
            param.requires_grad = False
            
        # è‡ªåŠ¨æ£€æµ‹éšè—å±‚ç»´åº¦
        print("ğŸ” æ­£åœ¨è‡ªåŠ¨æ£€æµ‹æ¨¡å‹è¾“å‡ºç»´åº¦...")
        dummy_s1 = torch.zeros(1, 10, dtype=torch.long)
        dummy_s2 = torch.zeros(1, 10, dtype=torch.long)
        
        with torch.no_grad():
            try:
                outputs = self.backbone(dummy_s1, dummy_s2)
                if hasattr(outputs, 'last_hidden_state'):
                    last_hidden = outputs.last_hidden_state
                elif isinstance(outputs, tuple):
                    last_hidden = outputs[0]
                else:
                    last_hidden = outputs
                
                self.hidden_size = last_hidden.shape[-1]
                print(f"âœ… æ£€æµ‹æˆåŠŸ! Hidden Size = {self.hidden_size}")
            except Exception as e:
                print(f"âš ï¸ æ£€æµ‹å¤±è´¥ ({e}), å›é€€åˆ°é»˜è®¤ 768")
                self.hidden_size = 768
        
        self.classifier = nn.Sequential(
            nn.Linear(self.hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 2)
        )

    def forward(self, s1_ids, s2_ids):
        outputs = self.backbone(s1_ids, s2_ids)
        
        if hasattr(outputs, 'last_hidden_state'):
            last_hidden_state = outputs.last_hidden_state
        elif isinstance(outputs, tuple):
            last_hidden_state = outputs[0]
        else:
            last_hidden_state = outputs
            
        last_token_feature = last_hidden_state[:, -1, :]
        logits = self.classifier(last_token_feature)
        return logits

# ================= ğŸ“¥ æ•°æ®é›†å®šä¹‰ =================
class InferenceDataset(Dataset):
    def __init__(self, df, tokenizer, seq_len=60, stride=1):
        self.tokenizer = tokenizer
        self.seq_len = seq_len
        self.samples = []
        
        # é¢„å¤„ç†
        self.df_values = df[['open', 'high', 'low', 'close', 'volume']].copy()
        self.meta_data = df[['datetime', 'close']].copy()
        
        # ç”Ÿæˆç´¢å¼• (stride=1 ä»£è¡¨å…¨è¦†ç›–)
        indices = range(seq_len, len(df), stride)
        print(f"ğŸ”ª æ­£åœ¨åˆ‡ç‰‡... (æ­¥é•¿: {stride}, é¢„è®¡ç”Ÿæˆ {len(indices)} ä¸ªæ ·æœ¬)")
        
        for i in indices:
            self.samples.append(i)

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        end_idx = self.samples[idx]
        start_idx = end_idx - self.seq_len
        
        df_segment = self.df_values.iloc[start_idx : end_idx]
        target_time = self.meta_data.iloc[end_idx-1]['datetime']
        target_price = self.meta_data.iloc[end_idx-1]['close']
        
        try:
            encoded = self.tokenizer.encode(df_segment)
            if isinstance(encoded, (tuple, list)) and len(encoded) == 2:
                s1_ids = encoded[0]
                s2_ids = encoded[1]
            else:
                s1_ids = encoded
                s2_ids = np.zeros_like(encoded)

            if isinstance(s1_ids, (np.ndarray, list)):
                s1_ids = torch.tensor(s1_ids, dtype=torch.long)
            if isinstance(s2_ids, (np.ndarray, list)):
                s2_ids = torch.tensor(s2_ids, dtype=torch.long)
                
            s1_ids = s1_ids.squeeze()
            s2_ids = s2_ids.squeeze()
        except Exception as e:
            s1_ids = torch.zeros(self.seq_len, dtype=torch.long)
            s2_ids = torch.zeros(self.seq_len, dtype=torch.long)
            
        return s1_ids, s2_ids, str(target_time), float(target_price)

# ================= ğŸš€ ä¸»ç¨‹åº =================
def main():
    print(f"ğŸ•µï¸â€â™€ï¸ å¯åŠ¨ AI å…¨å±€æ‰«æ (é«˜çµæ•åº¦æ¨¡å¼) | ç›®æ ‡: {TARGET_FILE}")
    
    if not os.path.exists(TARGET_FILE):
        print("âŒ æ–‡ä»¶ä¸å­˜åœ¨")
        return

    base_name = os.path.basename(TARGET_FILE).replace(".csv", "")
    ai_data_name = f"{base_name}_AI"
    ai_data_file = os.path.join("data", f"{ai_data_name}.csv")
    ai_label_file = os.path.join("labels", f"{ai_data_name}_labels.csv")

    print("âš™ï¸ åŠ è½½æ¨¡å‹ä¸­...")
    try:
        tokenizer = KronosTokenizer.from_pretrained(TOKENIZER_PATH)
        model = KronosClassifier(MODEL_PATH).to(DEVICE)
        
        if os.path.exists(WEIGHTS_PATH):
            state_dict = torch.load(WEIGHTS_PATH, map_location=DEVICE)
            model.classifier.load_state_dict(state_dict)
            print("âœ… æƒé‡åŠ è½½æˆåŠŸ")
        else:
            print("âŒ æ‰¾ä¸åˆ°æƒé‡æ–‡ä»¶ï¼Œè¯·å…ˆè®­ç»ƒï¼")
            return
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    model.eval()

    df = pd.read_csv(TARGET_FILE)
    df.columns = [c.lower() for c in df.columns]
    
    # æ„é€ æ•°æ®é›† (stride=1)
    dataset = InferenceDataset(df, tokenizer, seq_len=SEQ_LEN, stride=STRIDE)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)
    
    results = []
    print(f"ğŸš€ å¼€å§‹æ‰«æ {len(dataset)} ä¸ªçª—å£... (é˜ˆå€¼: {CONFIDENCE_THRESHOLD})")
    
    with torch.no_grad():
        for s1_ids, s2_ids, batch_times, batch_prices in tqdm(dataloader):
            s1_ids = s1_ids.to(DEVICE)
            s2_ids = s2_ids.to(DEVICE)
            
            logits = model(s1_ids, s2_ids)
            probs = torch.softmax(logits, dim=1)
            pos_probs = probs[:, 1].cpu().numpy()
            
            for i, prob in enumerate(pos_probs):
                if prob > CONFIDENCE_THRESHOLD:
                    results.append({
                        'datetime': batch_times[i],
                        'label': 1,
                        'price': batch_prices[i].item() if isinstance(batch_prices[i], torch.Tensor) else batch_prices[i],
                        'confidence': prob
                    })
    
    print(f"âœ… æ‰«æå®Œæˆï¼å‘ç° {len(results)} ä¸ªæ½œåœ¨æœºä¼šã€‚")
    
    if len(results) > 0:
        shutil.copy(TARGET_FILE, ai_data_file)
        print(f"ğŸ“‚ å·²åˆ›å»ºæ•°æ®å‰¯æœ¬: {ai_data_file}")
        
        res_df = pd.DataFrame(results)
        save_df = res_df[['datetime', 'label', 'price']]
        save_df.to_csv(ai_label_file, index=False)
        print(f"ğŸ’¾ å·²ä¿å­˜æ ‡æ³¨ç»“æœ: {ai_label_file}")
    else:
        print("ğŸ¤·â€â™‚ï¸ å³ä½¿é˜ˆå€¼é™åˆ°äº† 0.55ï¼Œè¿˜æ˜¯æ²¡æ‰¾åˆ°æœºä¼šã€‚è¯·æ£€æŸ¥è®­ç»ƒæ•°æ®æ˜¯å¦å¤ªå°‘ï¼Œæˆ–æ¨¡å‹æ˜¯å¦æ²¡æ”¶æ•›ã€‚")

if __name__ == "__main__":
    main()
