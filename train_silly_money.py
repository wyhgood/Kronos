import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from tqdm import tqdm

# ç¡®ä¿ model.py åœ¨å½“å‰ç›®å½•ä¸‹
from model import Kronos, KronosTokenizer 

# ================= âš™ï¸ é…ç½®åŒºåŸŸ =================
TOKENIZER_PATH = "NeoQuasar/Kronos-Tokenizer-base"
MODEL_PATH = "NeoQuasar/Kronos-base" # ç›´æ¥ä½¿ç”¨åŸå§‹ Base æ¨¡å‹

DATA_DIR = "data"
LABEL_DIR = "labels"

# é’ˆå¯¹ 89 ä¸ªæ ·æœ¬çš„å¾®è°ƒç­–ç•¥
BATCH_SIZE = 8        # æ ·æœ¬å°‘ï¼ŒBatch å°ä¸€ç‚¹ï¼Œæ›´æ–°æ¬¡æ•°å¤šä¸€ç‚¹
LEARNING_RATE = 1e-4  # åªè®­ç»ƒå¤´ï¼Œå­¦ä¹ ç‡å¯ä»¥ç»™å¤§ä¸€ç‚¹ (1e-4 æˆ– 5e-4)
EPOCHS = 50           # å¤šè·‘å‡ è½®ï¼Œä¿è¯æ”¶æ•›
SEQ_LEN = 60         
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ================= 1. æ•°æ®é€‚é…å™¨ (ä¿ç•™æ‰€æœ‰é˜²å´©é»‘ç§‘æŠ€) =================
class QuantLabelerDataset(Dataset):
    def __init__(self, data_dir, label_dir, tokenizer, seq_len=60):
        self.tokenizer = tokenizer
        self.seq_len = seq_len
        self.samples = [] 
        # Kronos-base ç‰©ç†è¯è¡¨é™åˆ¶ (éå¸¸é‡è¦ï¼)
        self.vocab_size = 1024 

        if not os.path.exists(label_dir):
            print(f"âš ï¸ é”™è¯¯: æ‰¾ä¸åˆ°æ ‡æ³¨æ–‡ä»¶å¤¹ {label_dir}")
            return
            
        label_files = [f for f in os.listdir(label_dir) if f.endswith("_labels.csv")]
        print(f"ğŸ”„ æ­£åœ¨æ‰«æçœŸå®æ ‡æ³¨æ•°æ®... å‘ç° {len(label_files)} ä¸ªæ–‡ä»¶")

        for l_file in label_files:
            symbol_key = l_file.replace("_labels.csv", "")
            raw_file = f"{symbol_key}.csv"
            raw_path = os.path.join(data_dir, raw_file)
            label_path = os.path.join(label_dir, l_file)

            if not os.path.exists(raw_path): continue

            try:
                df_raw = pd.read_csv(raw_path)
                df_label = pd.read_csv(label_path)
                df_raw['datetime'] = pd.to_datetime(df_raw['datetime'])
                df_label['datetime'] = pd.to_datetime(df_label['datetime'])
                df_raw.columns = [c.lower() for c in df_raw.columns]
                
                # ğŸ”¥ è‡ªåŠ¨è¡¥å…¨ Amount (é˜²æ­¢ 60x5 æŠ¥é”™)
                if 'amount' not in df_raw.columns:
                    df_raw['amount'] = df_raw['close'] * df_raw['volume']

                required_cols = ['open', 'high', 'low', 'close', 'volume', 'amount']
                
                for _, row in df_label.iterrows():
                    target_time = row['datetime']
                    label = int(row['label'])
                    matches = df_raw.index[df_raw['datetime'] == target_time].tolist()
                    if not matches: continue
                    idx = matches[0]
                    if idx < seq_len - 1: continue
                    
                    df_segment = df_raw.iloc[idx - seq_len + 1 : idx + 1][required_cols].copy()
                    
                    self.samples.append({
                        'values': df_segment.values.astype(np.float32),
                        'label': label
                    })
                
            except Exception as e:
                print(f"  è¯»å– {l_file} å‡ºé”™: {e}")

        print(f"âœ… çœŸå®æ•°æ®åŠ è½½å®Œæˆï¼å…± {len(self.samples)} ä¸ªæ ·æœ¬ã€‚")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        item = self.samples[idx]
        values = item['values'] 
        label = item['label']

        # ğŸ”¥ å½’ä¸€åŒ– (Log + Z-Score)
        # é˜²æ­¢æ•°å€¼è¿‡å¤§å¯¼è‡´ Tokenizer å†…éƒ¨æº¢å‡º
        norm_values = values.copy()
        norm_values[:, 4] = np.log1p(norm_values[:, 4]) 
        norm_values[:, 5] = np.log1p(norm_values[:, 5]) 
        price_mean = norm_values[:, :4].mean()
        price_std = norm_values[:, :4].std() + 1e-5
        norm_values[:, :4] = (norm_values[:, :4] - price_mean) / price_std

        input_tensor = torch.tensor(norm_values, dtype=torch.float32).unsqueeze(0)

        try:
            encoded = self.tokenizer.encode(input_tensor)
            if isinstance(encoded, (tuple, list)) and len(encoded) == 2:
                s1, s2 = encoded[0], encoded[1]
            else:
                s1, s2 = encoded, np.zeros_like(encoded)
            
            if isinstance(s1, (np.ndarray, list)): s1 = torch.tensor(s1, dtype=torch.long)
            if isinstance(s2, (np.ndarray, list)): s2 = torch.tensor(s2, dtype=torch.long)
            
            s1 = s1.squeeze()
            s2 = s2.squeeze()
            
            # ğŸ”¥ å–æ¨¡å¤§æ³• (Modulo Hack)
            # è§£å†³ ID è¶Šç•Œå¯¼è‡´ CUDA Error çš„ç»ˆææ–¹æ¡ˆ
            s1 = s1 % self.vocab_size
            s2 = s2 % self.vocab_size
            
        except Exception as e:
            s1 = torch.zeros(self.seq_len, dtype=torch.long)
            s2 = torch.zeros(self.seq_len, dtype=torch.long)

        return s1, s2, torch.tensor(label, dtype=torch.long)

# ================= 2. æ¨¡å‹å®šä¹‰ (åŸå§‹ Base + é”å¤´) =================
class KronosClassifier(nn.Module):
    def __init__(self, model_path):
        super().__init__()
        print(f"æ­£åœ¨åŠ è½½åŸå§‹ Base æ¨¡å‹: {model_path} ...")
        self.backbone = Kronos.from_pretrained(model_path)
        
        # ğŸ”¥ é”æ­»åº•åº§ (Frozen Backbone)
        # æˆ‘ä»¬åªä¿¡ä»»å®ƒçš„åŸå§‹ç›´è§‰ï¼Œä¸è®©å®ƒè¢«å°æ ·æœ¬å¸¦å
        print("ğŸ”’ æ­£åœ¨é”æ­»åº•åº§æ¨¡å‹å‚æ•° (Freeze)...")
        for param in self.backbone.parameters():
            param.requires_grad = False
            
        # è‡ªåŠ¨æ£€æµ‹ç»´åº¦
        dummy_s1 = torch.zeros(1, 10, dtype=torch.long)
        dummy_s2 = torch.zeros(1, 10, dtype=torch.long)
        with torch.no_grad():
            outputs = self.backbone(dummy_s1, dummy_s2)
            if hasattr(outputs, 'last_hidden_state'):
                last_hidden = outputs.last_hidden_state
            elif isinstance(outputs, tuple):
                last_hidden = outputs[0]
            else:
                last_hidden = outputs
            self.hidden_size = last_hidden.shape[-1]
        
        # æ–°çš„åˆ†ç±»å¤´ (Trainable)
        self.classifier = nn.Sequential(
            nn.Linear(self.hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.5), # é«˜ Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
            nn.Linear(256, 2)
        )

    def forward(self, s1, s2):
        # åº•åº§åªè´Ÿè´£æå–ç‰¹å¾
        outputs = self.backbone(s1, s2)
        
        if hasattr(outputs, 'last_hidden_state'):
            last_hidden = outputs.last_hidden_state
        elif isinstance(outputs, tuple):
            last_hidden = outputs[0]
        else:
            last_hidden = outputs

        # å–æœ€åä¸€ä¸ª Token çš„ç‰¹å¾è¿›åˆ†ç±»å¤´
        return self.classifier(last_hidden[:, -1, :])

# ================= 3. ä¸»è®­ç»ƒå¾ªç¯ =================
def main():
    print(f"ğŸš€ å¯åŠ¨çœŸå®æ•°æ®å¾®è°ƒ (åŸå§‹Base+é”å¤´) | è®¾å¤‡: {DEVICE}")
    
    # 1. å¿…é¡»é‡å¯ Python ç»ˆç«¯ä»¥æ¸…é™¤ä¹‹å‰çš„æ˜¾å­˜çŠ¶æ€
    
    tokenizer = KronosTokenizer.from_pretrained(TOKENIZER_PATH)
    train_dataset = QuantLabelerDataset(DATA_DIR, LABEL_DIR, tokenizer, seq_len=SEQ_LEN)
    
    if len(train_dataset) == 0:
        print("âŒ æ²¡æ•°æ®ï¼Œæ— æ³•è®­ç»ƒã€‚")
        return

    # å°æ ·æœ¬ç”¨å° Batch
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    model = KronosClassifier(MODEL_PATH).to(DEVICE)

    # ğŸ”¥ åªä¼˜åŒ– classifier (å¤´)
    optimizer = optim.AdamW(model.classifier.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()

    print(f"\nğŸ å¼€å§‹è®­ç»ƒ | æ ·æœ¬: {len(train_dataset)} | è½®æ•°: {EPOCHS}")
    
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        correct = 0
        total_samples = 0
        
        for s1, s2, batch_labels in train_loader:
            s1, s2, batch_labels = s1.to(DEVICE), s2.to(DEVICE), batch_labels.to(DEVICE)
            
            optimizer.zero_grad()
            logits = model(s1, s2)
            loss = criterion(logits, batch_labels)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            preds = torch.argmax(logits, dim=1)
            correct += (preds == batch_labels).sum().item()
            total_samples += batch_labels.size(0)
            
        epoch_acc = correct / total_samples
        print(f"Epoch {epoch+1:02d} | Loss: {total_loss/len(train_loader):.4f} | Acc: {epoch_acc:.2%}")
        
    save_path = "silly_money_base_raw.pth"
    torch.save(model.classifier.state_dict(), save_path)
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼åˆ†ç±»å¤´æƒé‡å·²ä¿å­˜ä¸º: {save_path}")

if __name__ == "__main__":
    main()
