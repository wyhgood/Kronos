import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from tqdm import tqdm

# ç¡®ä¿ model.py åœ¨å½“å‰ç›®å½•ä¸‹
from model import Kronos, KronosTokenizer 

# ================= é…ç½®åŒºåŸŸ =================
TOKENIZER_PATH = "NeoQuasar/Kronos-Tokenizer-base"
MODEL_PATH = "NeoQuasar/Kronos-small" 

DATA_DIR = "data"
LABEL_DIR = "labels"

BATCH_SIZE = 8       
LEARNING_RATE = 1e-4 
EPOCHS = 10          
SEQ_LEN = 60         
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ================= 1. æ•°æ®é€‚é…å™¨ (ä¿®å¤åŒæµè¾“å‡º) =================
class QuantLabelerDataset(Dataset):
    def __init__(self, data_dir, label_dir, tokenizer, seq_len=60):
        self.tokenizer = tokenizer
        self.seq_len = seq_len
        self.samples = [] 

        if not os.path.exists(label_dir):
            print(f"âš ï¸ é”™è¯¯: æ‰¾ä¸åˆ°æ ‡æ³¨æ–‡ä»¶å¤¹ {label_dir}")
            return
            
        label_files = [f for f in os.listdir(label_dir) if f.endswith("_labels.csv")]
        print(f"ğŸ”„ æ­£åœ¨æ‰«ææ•°æ®... å‘ç° {len(label_files)} ä¸ªæ ‡æ³¨æ–‡ä»¶")

        for l_file in label_files:
            symbol_key = l_file.replace("_labels.csv", "")
            raw_file = f"{symbol_key}.csv"
            raw_path = os.path.join(data_dir, raw_file)
            label_path = os.path.join(label_dir, l_file)

            if not os.path.exists(raw_path): continue

            try:
                df_raw = pd.read_csv(raw_path)
                df_label = pd.read_csv(label_path)
                df_raw['datetime'] = pd.to_datetime(df_raw['datetime'])
                df_label['datetime'] = pd.to_datetime(df_label['datetime'])
                
                # é¢„å…ˆæå–éœ€è¦çš„åˆ—ï¼Œç¡®ä¿åˆ—åå°å†™
                df_raw.columns = [c.lower() for c in df_raw.columns]
                
                for _, row in df_label.iterrows():
                    target_time = row['datetime']
                    label = int(row['label'])
                    matches = df_raw.index[df_raw['datetime'] == target_time].tolist()
                    if not matches: continue
                    idx = matches[0]
                    if idx < seq_len - 1: continue
                    
                    # æˆªå–
                    df_segment = df_raw.iloc[idx - seq_len + 1 : idx + 1].copy()
                    
                    self.samples.append({
                        'df': df_segment,
                        'label': label
                    })
                
            except Exception as e:
                print(f"  è¯»å– {l_file} å‡ºé”™: {e}")

        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼å…±æ„å»º {len(self.samples)} ä¸ªæœ‰æ•ˆæ ·æœ¬ã€‚")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        item = self.samples[idx]
        df = item['df']
        label = item['label']

        try:
            # --- ä¿®å¤: æ­£ç¡®å¤„ç† Tokenizer çš„åŒé‡è¾“å‡º (s1, s2) ---
            encoded = self.tokenizer.encode(df)
            
            # å¼ºåˆ¶è§£åŒ… tuple/list
            if isinstance(encoded, (tuple, list)) and len(encoded) == 2:
                s1_ids = encoded[0]
                s2_ids = encoded[1]
            else:
                # å®¹é”™å¤„ç†
                s1_ids = encoded
                s2_ids = np.zeros_like(encoded)

            # è½¬ Tensor
            if isinstance(s1_ids, (np.ndarray, list)):
                s1_ids = torch.tensor(s1_ids, dtype=torch.long)
            if isinstance(s2_ids, (np.ndarray, list)):
                s2_ids = torch.tensor(s2_ids, dtype=torch.long)
            
            s1_ids = s1_ids.squeeze()
            s2_ids = s2_ids.squeeze()
            
        except Exception as e:
            # print(f"Tokenizer error: {e}")
            s1_ids = torch.zeros(self.seq_len, dtype=torch.long)
            s2_ids = torch.zeros(self.seq_len, dtype=torch.long)

        return s1_ids, s2_ids, torch.tensor(label, dtype=torch.long)

# ================= 2. æ¨¡å‹å®šä¹‰ (ä¿®å¤ç»´åº¦ä¸è¾“å…¥) =================
class KronosClassifier(nn.Module):
    def __init__(self, model_path):
        super().__init__()
        print(f"æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_path} ...")
        self.backbone = Kronos.from_pretrained(model_path)
        
        # å†»ç»“å‚æ•°
        for param in self.backbone.parameters():
            param.requires_grad = False
            
        # --- ä¿®å¤: è‡ªåŠ¨æ£€æµ‹éšè—å±‚ç»´åº¦ ---
        print("ğŸ” æ­£åœ¨è‡ªåŠ¨æ£€æµ‹æ¨¡å‹è¾“å‡ºç»´åº¦...")
        dummy_s1 = torch.zeros(1, 10, dtype=torch.long) # æ„é€ å‡æ•°æ®
        dummy_s2 = torch.zeros(1, 10, dtype=torch.long)
        
        with torch.no_grad():
            try:
                outputs = self.backbone(dummy_s1, dummy_s2)
                if hasattr(outputs, 'last_hidden_state'):
                    last_hidden = outputs.last_hidden_state
                elif isinstance(outputs, tuple):
                    last_hidden = outputs[0]
                else:
                    last_hidden = outputs
                
                self.hidden_size = last_hidden.shape[-1]
                print(f"âœ… æ£€æµ‹æˆåŠŸ! Hidden Size = {self.hidden_size}")
            except Exception as e:
                print(f"âš ï¸ æ£€æµ‹å¤±è´¥ ({e}), å›é€€åˆ°é»˜è®¤ 768")
                self.hidden_size = 768
        
        # å®šä¹‰åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(self.hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 2)
        )

    def forward(self, s1_ids, s2_ids):
        # --- ä¿®å¤: ä¼ å…¥åŒæµå‚æ•° ---
        outputs = self.backbone(s1_ids, s2_ids)
        
        if hasattr(outputs, 'last_hidden_state'):
            last_hidden_state = outputs.last_hidden_state
        elif isinstance(outputs, tuple):
            last_hidden_state = outputs[0]
        else:
            last_hidden_state = outputs

        # å–æœ€åä¸€ä¸ª Token
        last_token_feature = last_hidden_state[:, -1, :]
        logits = self.classifier(last_token_feature)
        return logits

# ================= 3. ä¸»è®­ç»ƒå¾ªç¯ =================
def main():
    print(f"ğŸš€ å¯åŠ¨è®­ç»ƒä»»åŠ¡ | è®¾å¤‡: {DEVICE}")
    
    print("æ­£åœ¨åŠ è½½ Tokenizer...")
    try:
        tokenizer = KronosTokenizer.from_pretrained(TOKENIZER_PATH)
        print("âœ… Tokenizer åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Tokenizer åŠ è½½å¤±è´¥: {e}")
        return

    train_dataset = QuantLabelerDataset(DATA_DIR, LABEL_DIR, tokenizer, seq_len=SEQ_LEN)
    
    if len(train_dataset) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ ·æœ¬")
        return

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

    model = KronosClassifier(MODEL_PATH).to(DEVICE)
    optimizer = optim.AdamW(model.classifier.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()

    print(f"\nğŸ å¼€å§‹è®­ç»ƒ | æ ·æœ¬æ•°: {len(train_dataset)} | è½®æ•°: {EPOCHS}")
    
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        correct = 0
        total_samples = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}")
        
        # --- ä¿®å¤: è§£åŒ… s1, s2, label ---
        for s1_ids, s2_ids, batch_labels in progress_bar:
            s1_ids = s1_ids.to(DEVICE)
            s2_ids = s2_ids.to(DEVICE)
            batch_labels = batch_labels.to(DEVICE)
            
            optimizer.zero_grad()
            
            # ä¼ å…¥åŒæµ
            logits = model(s1_ids, s2_ids)
            
            loss = criterion(logits, batch_labels)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            preds = torch.argmax(logits, dim=1)
            correct += (preds == batch_labels).sum().item()
            total_samples += batch_labels.size(0)
            
            acc_str = f"{correct/total_samples:.2%}" if total_samples > 0 else "0.00%"
            progress_bar.set_postfix({'Loss': f"{loss.item():.4f}", 'Acc': acc_str})
        
    torch.save(model.classifier.state_dict(), "silly_money_head.pth")
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼åˆ†ç±»å¤´å·²ä¿å­˜ä¸º silly_money_head.pth")

if __name__ == "__main__":
    main()
