import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from transformers import AutoModel, AutoTokenizer
from tqdm import tqdm # è¿›åº¦æ¡åº“ï¼Œå¦‚æœæ²¡æœ‰è¯· pip install tqdm

# ================= é…ç½®åŒºåŸŸ =================
# æ¨¡å‹è·¯å¾„ (è¯·ç¡®ä¿è¿™æ˜¯ä½  download_weights.py ä¸‹è½½çš„è·¯å¾„)
MODEL_PATH = "./checkpoints/Kronos-small"

# æ•°æ®è·¯å¾„ (å¯¹åº”ä½ çš„ Streamlit ç›®å½•ç»“æ„)
DATA_DIR = "data"
LABEL_DIR = "labels"

# è®­ç»ƒå‚æ•°
BATCH_SIZE = 8       # å¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œæ”¹å°ä¸€ç‚¹ï¼Œæ¯”å¦‚ 4 æˆ– 2
LEARNING_RATE = 1e-4 # å¾®è°ƒé€šå¸¸ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
EPOCHS = 10          # è®­ç»ƒè½®æ•°
SEQ_LEN = 60         # å›çœ‹çª—å£é•¿åº¦ (å’Œä½ çš„é€»è¾‘ä¸€è‡´)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ================= 1. æ•°æ®é€‚é…å™¨ (ç›´æ¥å¯¹æ¥ä½ çš„æ ‡æ³¨å·¥å…·) =================
class QuantLabelerDataset(Dataset):
    def __init__(self, data_dir, label_dir, tokenizer, seq_len=60):
        self.tokenizer = tokenizer
        self.seq_len = seq_len
        self.samples = [] 

        # æ‰«ææ‰€æœ‰æ ‡æ³¨æ–‡ä»¶
        if not os.path.exists(label_dir):
            print(f"âš ï¸ é”™è¯¯: æ‰¾ä¸åˆ°æ ‡æ³¨æ–‡ä»¶å¤¹ {label_dir}")
            return
            
        label_files = [f for f in os.listdir(label_dir) if f.endswith("_labels.csv")]
        print(f"ğŸ”„ æ­£åœ¨æ‰«ææ•°æ®... å‘ç° {len(label_files)} ä¸ªæ ‡æ³¨æ–‡ä»¶")

        for l_file in label_files:
            # è§£æå“ç§å (ä¾‹å¦‚ "rb_labels.csv" -> "rb.csv")
            symbol_key = l_file.replace("_labels.csv", "")
            raw_file = f"{symbol_key}.csv"
            raw_path = os.path.join(data_dir, raw_file)
            label_path = os.path.join(label_dir, l_file)

            if not os.path.exists(raw_path):
                print(f"  è·³è¿‡ {l_file}: æ‰¾ä¸åˆ°å¯¹åº”çš„åŸå§‹è¡Œæƒ…æ–‡ä»¶ {raw_file}")
                continue

            # åŠ è½½æ•°æ®
            try:
                df_raw = pd.read_csv(raw_path)
                df_label = pd.read_csv(label_path)
                
                # ç»Ÿä¸€æ—¶é—´æ ¼å¼
                df_raw['datetime'] = pd.to_datetime(df_raw['datetime'])
                df_label['datetime'] = pd.to_datetime(df_label['datetime'])
                
                # éå†æ ‡æ³¨ç‚¹
                count = 0
                for _, row in df_label.iterrows():
                    target_time = row['datetime']
                    label = int(row['label'])

                    # æŸ¥æ‰¾å¯¹åº”çš„æ—¶é—´ç´¢å¼•
                    matches = df_raw.index[df_raw['datetime'] == target_time].tolist()
                    if not matches: continue
                    
                    idx = matches[0]

                    # ç¡®ä¿å‰é¢æœ‰è¶³å¤Ÿçš„æ•°æ® (60æ ¹)
                    if idx < seq_len - 1: continue
                    
                    # æˆªå– DataFrameç‰‡æ®µ (åŒ…å«å½“å‰æ ¹)
                    # èŒƒå›´: [idx - 59, idx] å…± 60 æ ¹
                    df_segment = df_raw.iloc[idx - seq_len + 1 : idx + 1].copy()
                    
                    self.samples.append({
                        'df': df_segment,
                        'label': label
                    })
                    count += 1
                # print(f"  {symbol_key}: åŠ è½½äº† {count} ä¸ªæ ·æœ¬")
                
            except Exception as e:
                print(f"  è¯»å– {l_file} å‡ºé”™: {e}")

        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼å…±æ„å»º {len(self.samples)} ä¸ªæœ‰æ•ˆæ ·æœ¬ã€‚")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        item = self.samples[idx]
        df = item['df']
        label = item['label']

        # --- è°ƒç”¨ Kronos Tokenizer ---
        # æ³¨æ„: Kronos Tokenizer é€šå¸¸éœ€è¦ pandas DataFrame ä½œä¸ºè¾“å…¥
        # å®ƒä¼šè‡ªåŠ¨è¯†åˆ« open, high, low, close, volume åˆ—
        try:
            # å®˜æ–¹ API è°ƒç”¨æ–¹å¼
            input_ids = self.tokenizer.encode(df)
            
            # å¦‚æœ tokenizer è¿”å›çš„æ˜¯ listï¼Œè½¬ tensor
            if isinstance(input_ids, list):
                input_ids = torch.tensor(input_ids, dtype=torch.long)
            elif isinstance(input_ids, np.ndarray):
                input_ids = torch.from_numpy(input_ids).long()
                
            # ç¡®ä¿å»æ‰å¤šä½™çš„ batch ç»´åº¦ (å¦‚æœæœ‰)
            input_ids = input_ids.squeeze()
            
        except Exception as e:
            print(f"Tokenizer ç¼–ç é”™è¯¯: {e}")
            # å‡ºé”™æ—¶è¿”å›ä¸€ä¸ªå…¨0çš„tensoré˜²æ­¢ç¨‹åºå´©æºƒ
            input_ids = torch.zeros(self.seq_len, dtype=torch.long)

        return input_ids, torch.tensor(label, dtype=torch.long)

# ================= 2. æ¨¡å‹å®šä¹‰ (å†»ç»“éª¨æ¶ + åˆ†ç±»å¤´) =================
class KronosClassifier(nn.Module):
    def __init__(self, pretrained_path):
        super().__init__()
        print(f"æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {pretrained_path} ...")
        
        # åŠ è½½åº•åº§ (Transformer)
        self.backbone = AutoModel.from_pretrained(
            pretrained_path, 
            trust_remote_code=True
        )
        
        # --- å†»ç»“åº•åº§å‚æ•° (Linear Probing) ---
        # è¿™æ ·æˆ‘ä»¬åªè®­ç»ƒåˆ†ç±»å¤´ï¼Œä¿æŠ¤åº•åº§çš„â€œé€šè¯†â€
        for param in self.backbone.parameters():
            param.requires_grad = False
            
        # è·å–éšè—å±‚ç»´åº¦ (Kronos-small é€šå¸¸æ˜¯ 768ï¼ŒBase æ˜¯ 1024)
        # å°è¯•ä» config è¯»å–ï¼Œè¯»ä¸åˆ°å°±é»˜è®¤ 768
        try:
            self.hidden_size = self.backbone.config.hidden_size
        except:
            self.hidden_size = 768 
            
        print(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œéšè—å±‚ç»´åº¦: {self.hidden_size}ï¼Œåº•åº§å·²å†»ç»“ã€‚")
        
        # --- å®šä¹‰åˆ†ç±»å¤´ ---
        self.classifier = nn.Sequential(
            nn.Linear(self.hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.3),     # é˜²æ­¢è¿‡æ‹Ÿåˆ
            nn.Linear(256, 2)    # è¾“å‡º: [No_Intent, Yes_Intent]
        )

    def forward(self, input_ids):
        # Kronos æ˜¯ decoder-onlyï¼Œè¾“å…¥ input_ids å³å¯
        # output.last_hidden_state å½¢çŠ¶: [batch, seq_len, hidden]
        outputs = self.backbone(input_ids=input_ids)
        
        # æˆ‘ä»¬åªå–åºåˆ—çš„æœ€åä¸€ä¸ª Token çš„ç‰¹å¾
        # å› ä¸ºåœ¨è‡ªå›å½’æ¨¡å‹ä¸­ï¼Œæœ€åä¸€ä¸ª Token åŒ…å«äº†æ•´ä¸ªåºåˆ—çš„ä¿¡æ¯
        last_token_feature = outputs.last_hidden_state[:, -1, :]
        
        # è¿‡åˆ†ç±»å¤´
        logits = self.classifier(last_token_feature)
        return logits

# ================= 3. ä¸»è®­ç»ƒå¾ªç¯ =================
def main():
    print(f"ğŸš€ å¯åŠ¨è®­ç»ƒä»»åŠ¡ | è®¾å¤‡: {DEVICE}")
    
    # A. åŠ è½½ Tokenizer
    print("æ­£åœ¨åŠ è½½ Tokenizer...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    except Exception as e:
        print(f"âŒ Tokenizer åŠ è½½å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥ MODEL_PATH æ˜¯å¦æ­£ç¡®ï¼Œæˆ–è€…ç½‘ç»œæ˜¯å¦é€šç•…ã€‚")
        return

    # B. å‡†å¤‡æ•°æ®é›†
    train_dataset = QuantLabelerDataset(DATA_DIR, LABEL_DIR, tokenizer, seq_len=SEQ_LEN)
    
    if len(train_dataset) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ ·æœ¬ï¼Œè¯·å…ˆå» Streamlit æ ‡æ³¨ä¸€äº›æ•°æ®ï¼")
        return

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

    # C. åˆå§‹åŒ–æ¨¡å‹
    model = KronosClassifier(MODEL_PATH).to(DEVICE)

    # D. å®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    # åªä¼˜åŒ– classifier çš„å‚æ•°ï¼
    optimizer = optim.AdamW(model.classifier.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss() # è‡ªåŠ¨å¤„ç† Softmax

    # E. å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ å¼€å§‹è®­ç»ƒ | æ ·æœ¬æ•°: {len(train_dataset)} | æ‰¹æ¬¡å¤§å°: {BATCH_SIZE} | è½®æ•°: {EPOCHS}")
    
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        correct = 0
        total_samples = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}")
        
        for batch_ids, batch_labels in progress_bar:
            batch_ids = batch_ids.to(DEVICE)
            batch_labels = batch_labels.to(DEVICE)
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            logits = model(batch_ids)
            
            # è®¡ç®—æŸå¤±
            loss = criterion(logits, batch_labels)
            
            # åå‘ä¼ æ’­
            loss.backward()
            optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            preds = torch.argmax(logits, dim=1)
            correct += (preds == batch_labels).sum().item()
            total_samples += batch_labels.size(0)
            
            # æ›´æ–°è¿›åº¦æ¡
            current_acc = correct / total_samples
            progress_bar.set_postfix({'Loss': f"{loss.item():.4f}", 'Acc': f"{current_acc:.2%}"})
        
        avg_loss = total_loss / len(train_loader)
        epoch_acc = correct / total_samples
        print(f"ğŸ“Š Epoch {epoch+1} ç»“æŸ | å¹³å‡ Loss: {avg_loss:.4f} | å‡†ç¡®ç‡: {epoch_acc:.2%}")

    # F. ä¿å­˜æ¨¡å‹ (åªä¿å­˜åˆ†ç±»å¤´ï¼Œå› ä¸ºåº•åº§æ²¡å˜ï¼Œè¿™æ ·æ–‡ä»¶å¾ˆå°)
    save_path = "silly_money_head.pth"
    torch.save(model.classifier.state_dict(), save_path)
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼åˆ†ç±»å¤´å·²ä¿å­˜ä¸º: {save_path}")
    print("ğŸ’¡ æç¤º: æ¨ç†æ—¶ï¼ŒåŠ è½½åº•åº§åï¼Œå†ç”¨ load_state_dict åŠ è½½è¿™ä¸ªæ–‡ä»¶å³å¯ã€‚")

if __name__ == "__main__":
    main()
