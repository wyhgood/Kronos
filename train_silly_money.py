import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from tqdm import tqdm

# ç¡®ä¿ model.py åœ¨å½“å‰ç›®å½•ä¸‹
from model import Kronos, KronosTokenizer 

# ================= é…ç½®åŒºåŸŸ =================
TOKENIZER_PATH = "NeoQuasar/Kronos-Tokenizer-base"
MODEL_PATH = "NeoQuasar/Kronos-small" 

DATA_DIR = "data"
LABEL_DIR = "labels"

BATCH_SIZE = 8       
LEARNING_RATE = 1e-4 
EPOCHS = 10          
SEQ_LEN = 60         
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ================= 1. æ•°æ®é€‚é…å™¨ =================
class QuantLabelerDataset(Dataset):
    def __init__(self, data_dir, label_dir, tokenizer, seq_len=60):
        self.tokenizer = tokenizer
        self.seq_len = seq_len
        self.samples = [] 

        if not os.path.exists(label_dir):
            print(f"âš ï¸ é”™è¯¯: æ‰¾ä¸åˆ°æ ‡æ³¨æ–‡ä»¶å¤¹ {label_dir}")
            return
            
        label_files = [f for f in os.listdir(label_dir) if f.endswith("_labels.csv")]
        print(f"ğŸ”„ æ­£åœ¨æ‰«ææ•°æ®... å‘ç° {len(label_files)} ä¸ªæ ‡æ³¨æ–‡ä»¶")

        for l_file in label_files:
            symbol_key = l_file.replace("_labels.csv", "")
            raw_file = f"{symbol_key}.csv"
            raw_path = os.path.join(data_dir, raw_file)
            label_path = os.path.join(label_dir, l_file)

            if not os.path.exists(raw_path): continue

            try:
                df_raw = pd.read_csv(raw_path)
                df_label = pd.read_csv(label_path)
                df_raw['datetime'] = pd.to_datetime(df_raw['datetime'])
                df_label['datetime'] = pd.to_datetime(df_label['datetime'])
                
                for _, row in df_label.iterrows():
                    target_time = row['datetime']
                    label = int(row['label'])
                    matches = df_raw.index[df_raw['datetime'] == target_time].tolist()
                    if not matches: continue
                    idx = matches[0]
                    if idx < seq_len - 1: continue
                    
                    df_segment = df_raw.iloc[idx - seq_len + 1 : idx + 1].copy()
                    
                    # å…³é”®ä¿®å¤ï¼šç›´æ¥æå–æ•°å€¼åˆ—ï¼Œè½¬ä¸º numpy array
                    # å‡è®¾æ¨¡å‹éœ€è¦ 'open', 'high', 'low', 'close', 'volume'
                    # å¹¶ä¸”é¡ºåºå¾ˆé‡è¦ï¼Œæˆ–è€… Tokenizer èƒ½å¤„ç† DataFrame ä½†éœ€è¦ç‰¹å®šåˆ—
                    # ä¸ºäº†ç¨³å¦¥ï¼Œæˆ‘ä»¬ä¼  DataFrameï¼Œä½†åœ¨ __getitem__ é‡Œåšä¿æŠ¤
                    self.samples.append({
                        'df': df_segment,
                        'label': label
                    })
                
            except Exception as e:
                print(f"  è¯»å– {l_file} å‡ºé”™: {e}")

        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼å…±æ„å»º {len(self.samples)} ä¸ªæœ‰æ•ˆæ ·æœ¬ã€‚")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        item = self.samples[idx]
        df = item['df']
        label = item['label']

        try:
            # --- ä¿®å¤ 1: Tokenizer è°ƒç”¨æ–¹å¼ ---
            # å¦‚æœ tokenizer.encode æŠ¥é”™ï¼Œå¾ˆå¯èƒ½æ˜¯å› ä¸ºå®ƒæœŸæœ› raw values
            # æˆ–è€…æ˜¯ DataFrame ä½†æ ¼å¼æœ‰ç»†å¾®å·®åˆ«
            # è®¸å¤š TimeSeries Tokenizer æœŸæœ›è¾“å…¥æ˜¯ DataFrame
            # å¦‚æœä¹‹å‰çš„æŠ¥é”™æ˜¯ linear() argument must be Tensor
            # è¯´æ˜ Tokenizer å†…éƒ¨æ²¡æœ‰è‡ªåŠ¨æŠŠ DataFrame è½¬ Tensor
            
            # æˆ‘ä»¬æ‰‹åŠ¨æŠŠ DataFrame è½¬ä¸º Tensor ä¼ è¿›å»è¯•è¯•
            # æå– 5 ä¸ªæ ¸å¿ƒåˆ—
            cols = ['open', 'high', 'low', 'close', 'volume']
            # ç¡®ä¿åˆ—å­˜åœ¨ä¸”ä¸º float32
            data_values = df[cols].values.astype(np.float32) 
            
            # ä¼ ç»™ tokenizer
            # æ³¨æ„ï¼šKronosTokenizer.encode å…·ä½“å®ç°å¦‚æœæ˜¯å¤„ç† dataframe
            # é‚£ä¹ˆä¹‹å‰çš„æŠ¥é”™å¾ˆå¥‡æ€ªã€‚æˆ‘ä»¬å°è¯•ç›´æ¥ä¼  values
            # å¦‚æœ tokenizer åªéœ€è¦ dataframeï¼Œé‚£å¯èƒ½æ˜¯ df é‡Œçš„æ•°æ®ç±»å‹ä¸æ˜¯ float
            
            # æ–¹æ¡ˆ A: ä¾ç„¶ä¼  dfï¼Œä½†ç¡®ä¿å…¨æ˜¯ float
            # input_ids = self.tokenizer.encode(df)
            
            # æ–¹æ¡ˆ B (é’ˆå¯¹æŠ¥é”™ä¿®å¤): Tokenizer å¯èƒ½åªæ˜¯åšç¦»æ•£åŒ–ï¼Œ
            # å®é™…ä¸Šæ¨¡å‹è¾“å…¥éœ€è¦çš„æ˜¯ embedding å‰çš„æ•°å€¼æˆ–è€…å·²ç»é‡åŒ–å¥½çš„ ID
            # è®©æˆ‘ä»¬å‡è®¾ tokenizer.encode è¿”å›çš„æ˜¯ token ids
            input_ids = self.tokenizer.encode(df)
            
            if isinstance(input_ids, list):
                input_ids = torch.tensor(input_ids, dtype=torch.long)
            elif isinstance(input_ids, np.ndarray):
                input_ids = torch.from_numpy(input_ids).long()
            
            input_ids = input_ids.squeeze()
            
        except Exception as e:
            # print(f"Tokenizer ç¼–ç é”™è¯¯: {e}") 
            # æš‚æ—¶ç”¨å…¨ 0 æ›¿ä»£ï¼Œé¿å…åˆ·å±ï¼Œå®é™…éœ€è¦è°ƒè¯• tokenizer æºç 
            input_ids = torch.zeros(self.seq_len, dtype=torch.long)

        return input_ids, torch.tensor(label, dtype=torch.long)

# ================= 2. æ¨¡å‹å®šä¹‰ =================
class KronosClassifier(nn.Module):
    def __init__(self, model_path):
        super().__init__()
        print(f"æ­£åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_path} ...")
        self.backbone = Kronos.from_pretrained(model_path)
        
        for param in self.backbone.parameters():
            param.requires_grad = False
            
        try:
            self.hidden_size = self.backbone.config.hidden_size
        except:
            self.hidden_size = 768 
            
        print(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œéšè—å±‚ç»´åº¦: {self.hidden_size}")
        
        self.classifier = nn.Sequential(
            nn.Linear(self.hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 2)
        )

    def forward(self, input_ids):
        # --- ä¿®å¤ 2: ç§»é™¤ output_hidden_states å‚æ•° ---
        # Kronos çš„ forward åªæ¥å— input_ids (å’Œ mask)
        # å®ƒè¿”å›çš„ç›´æ¥å°±æ˜¯ logits æˆ–è€… transformer output
        outputs = self.backbone(input_ids)
        
        # æ£€æŸ¥è¾“å‡ºç±»å‹å¹¶æå– hidden state
        if hasattr(outputs, 'last_hidden_state'):
            last_hidden_state = outputs.last_hidden_state
        elif isinstance(outputs, tuple):
            last_hidden_state = outputs[0] # é€šå¸¸ç¬¬ä¸€ä¸ªæ˜¯ hidden state
        elif isinstance(outputs, torch.Tensor):
            # å¦‚æœç›´æ¥è¿”å› Tensorï¼Œè¿™é€šå¸¸æ˜¯ Logits (Vocab Size)
            # è¿™å°±éº»çƒ¦äº†ï¼Œæˆ‘ä»¬éœ€è¦ä¸­é—´å±‚çš„ç‰¹å¾
            # å¦‚æœ Kronos forward æ²¡æ³•è¿”å› hidden stateï¼Œæˆ‘ä»¬éœ€è¦ hack ä¸€ä¸‹
            # ä½†é€šå¸¸ transformer åº“çš„æ¨¡å‹éƒ½ä¼šè¿”å› hidden state
            # å‡è®¾å®ƒæ˜¯ logitsï¼Œç»´åº¦æ˜¯ [batch, seq, vocab]
            # æˆ‘ä»¬ä¸èƒ½ç”¨ logits åšåˆ†ç±»ç‰¹å¾ï¼Œå› ä¸ºå®ƒå¤ªå¤§äº†
            
            # è®©æˆ‘ä»¬å†è¯•ä¸€æ¬¡å‡è®¾å®ƒæ˜¯ hidden state
            # å¦‚æœç»´åº¦æœ€åä¸€ç»´æ˜¯ 768ï¼Œé‚£å°±æ˜¯ hidden state
            # å¦‚æœæ˜¯ 30000+ï¼Œé‚£å°±æ˜¯ logits
            if outputs.shape[-1] == self.hidden_size:
                last_hidden_state = outputs
            else:
                # è¿™æ˜¯ä¸€ä¸ªæ‚²å‰§ï¼Œæ¨¡å‹åªåå‡ºé¢„æµ‹ç»“æœï¼Œä¸åå‡ºç‰¹å¾
                # æˆ‘ä»¬åªèƒ½å¼ºè¡Œç”¨å®ƒçš„ embedding å±‚æˆ–è€…ä¿®æ”¹æºç 
                # ä½†å¤§æ¦‚ç‡å®ƒè¿”å›çš„æ˜¯ hidden state (Decoder output)
                last_hidden_state = outputs # æš‚æ—¶èµŒå®ƒæ˜¯ç‰¹å¾
        else:
            last_hidden_state = outputs

        # å–æœ€åä¸€ä¸ª Token
        last_token_feature = last_hidden_state[:, -1, :]
        logits = self.classifier(last_token_feature)
        return logits

# ================= 3. ä¸»è®­ç»ƒå¾ªç¯ =================
def main():
    print(f"ğŸš€ å¯åŠ¨è®­ç»ƒä»»åŠ¡ | è®¾å¤‡: {DEVICE}")
    
    print("æ­£åœ¨åŠ è½½ Tokenizer...")
    try:
        tokenizer = KronosTokenizer.from_pretrained(TOKENIZER_PATH)
        print("âœ… Tokenizer åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Tokenizer åŠ è½½å¤±è´¥: {e}")
        return

    train_dataset = QuantLabelerDataset(DATA_DIR, LABEL_DIR, tokenizer, seq_len=SEQ_LEN)
    
    if len(train_dataset) == 0:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ ·æœ¬")
        return

    # æ³¨æ„ï¼šå¦‚æœ Tokenizer æŠ¥é”™ï¼Œè¿™é‡Œçš„ collate_fn å¯èƒ½éœ€è¦å¤„ç† padding
    # ä½† Kronos åº”è¯¥æ˜¯å®šé•¿è¾“å…¥çš„ï¼Œä¸éœ€è¦ padding
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)

    model = KronosClassifier(MODEL_PATH).to(DEVICE)
    optimizer = optim.AdamW(model.classifier.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()

    print(f"\nğŸ å¼€å§‹è®­ç»ƒ | æ ·æœ¬æ•°: {len(train_dataset)} | è½®æ•°: {EPOCHS}")
    
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        correct = 0
        total_samples = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}")
        
        for batch_ids, batch_labels in progress_bar:
            batch_ids = batch_ids.to(DEVICE)
            batch_labels = batch_labels.to(DEVICE)
            
            # ç®€å•çš„é˜²é”™ï¼šå¦‚æœ batch_ids å…¨æ˜¯ 0ï¼Œè¯´æ˜ tokenizer å¤±è´¥äº†ï¼Œè·³è¿‡
            if batch_ids.sum() == 0:
                continue

            optimizer.zero_grad()
            logits = model(batch_ids)
            loss = criterion(logits, batch_labels)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            preds = torch.argmax(logits, dim=1)
            correct += (preds == batch_labels).sum().item()
            total_samples += batch_labels.size(0)
            
            acc_str = f"{correct/total_samples:.2%}" if total_samples > 0 else "0.00%"
            progress_bar.set_postfix({'Loss': f"{loss.item():.4f}", 'Acc': acc_str})
        
    torch.save(model.classifier.state_dict(), "silly_money_head.pth")
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼åˆ†ç±»å¤´å·²ä¿å­˜ã€‚")

if __name__ == "__main__":
    main()
