import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
from tqdm import tqdm
from model import Kronos, KronosTokenizer 

# ================= âš™ï¸ é…ç½® =================
TRAIN_DATA = "synthetic_data/train.npy"
VAL_DATA = "synthetic_data/val.npy"
TOKENIZER_PATH = "NeoQuasar/Kronos-Tokenizer-base"
MODEL_PATH = "NeoQuasar/Kronos-base" 

BATCH_SIZE = 64      
EPOCHS = 5           
LEARNING_RATE = 2e-5 
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ================= ğŸ“¥ æ•°æ®é›† (å–æ¨¡ä¿®å¤ç‰ˆ) =================
class SyntheticDataset(Dataset):
    def __init__(self, npy_path, tokenizer):
        print(f"ğŸ“¥ åŠ è½½ {npy_path} ...")
        self.data = np.load(npy_path, allow_pickle=True)
        self.tokenizer = tokenizer
        
        # ğŸ”¥ æ ¹æ®ä¹‹å‰çš„æ£€æŸ¥ï¼Œè¯è¡¨åªæœ‰ 1024
        self.vocab_size = 1024 

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        values = item['df'].astype(np.float32) 
        label = item['label']
        
        # 1. å½’ä¸€åŒ– (ä¿æŒä¸å˜ï¼Œè¿™ä¾ç„¶å¾ˆæœ‰å¿…è¦)
        norm_values = values.copy()
        norm_values[:, 4] = np.log1p(norm_values[:, 4]) # Volume
        norm_values[:, 5] = np.log1p(norm_values[:, 5]) # Amount
        price_mean = norm_values[:, :4].mean()
        price_std = norm_values[:, :4].std() + 1e-5
        norm_values[:, :4] = (norm_values[:, :4] - price_mean) / price_std
        
        # 2. è½¬ Tensor [1, 60, 6]
        input_tensor = torch.tensor(norm_values, dtype=torch.float32).unsqueeze(0)
        
        # 3. Tokenize
        try:
            encoded = self.tokenizer.encode(input_tensor)
            if isinstance(encoded, (tuple, list)) and len(encoded) == 2:
                s1, s2 = encoded[0], encoded[1]
            else:
                s1, s2 = encoded, np.zeros_like(encoded)
        except:
            s1 = torch.zeros(60, dtype=torch.long)
            s2 = torch.zeros(60, dtype=torch.long)

        # 4. åå¤„ç†
        if isinstance(s1, (np.ndarray, list)): s1 = torch.tensor(s1, dtype=torch.long)
        if isinstance(s2, (np.ndarray, list)): s2 = torch.tensor(s2, dtype=torch.long)
        
        s1 = s1.squeeze()
        s2 = s2.squeeze()
        
        # ğŸ”¥ğŸ”¥ğŸ”¥ã€ç¥æ¥ä¹‹ç¬”ï¼šå–æ¨¡ Hackã€‘ğŸ”¥ğŸ”¥ğŸ”¥
        # ä¸å†ç”¨ clamp(åˆ‡é™¤)ï¼Œè€Œæ˜¯ç”¨ % (æŠ˜å )
        # è¿™æ · 50000 ä¸ä¼šå˜æˆ 1023ï¼Œè€Œæ˜¯å˜æˆ 832 (ä¸¾ä¾‹)
        # 50001 ä¼šå˜æˆ 833
        # æ•°æ®å·®å¼‚è¢«ä¿ç•™ä¸‹æ¥äº†ï¼Œè€Œä¸”ç»å¯¹ä¸ä¼šè¶Šç•ŒæŠ¥é”™ï¼
        s1 = s1 % self.vocab_size
        s2 = s2 % self.vocab_size
             
        return s1, s2, torch.tensor(label, dtype=torch.long)

# ================= ğŸ—ï¸ æ¨¡å‹å®šä¹‰ =================
class KronosClassifier(nn.Module):
    def __init__(self, model_path):
        super().__init__()
        print(f"æ­£åœ¨åŠ è½½å¤§æ¨¡å‹: {model_path} ...")
        self.backbone = Kronos.from_pretrained(model_path)
        
        print("ğŸ”“ åº•åº§æ¨¡å‹å·²è§£é”ï¼Œè¿›è¡Œå…¨é‡å¾®è°ƒ...")
        print(f"ğŸ”„ å¯ç”¨ Modulo Hack: å°† Token ID æŠ˜å è‡³ [0, 1023]")
            
        print("ğŸ” æ­£åœ¨æ£€æµ‹ Base æ¨¡å‹ç»´åº¦...")
        dummy_s1 = torch.zeros(1, 10, dtype=torch.long)
        dummy_s2 = torch.zeros(1, 10, dtype=torch.long)
        
        with torch.no_grad():
            outputs = self.backbone(dummy_s1, dummy_s2)
            # å…¼å®¹å„ç§è¾“å‡ºæ ¼å¼
            if hasattr(outputs, 'last_hidden_state'):
                last_hidden = outputs.last_hidden_state
            elif isinstance(outputs, tuple):
                last_hidden = outputs[0]
            else:
                last_hidden = outputs
            
            self.hidden_size = last_hidden.shape[-1]
            print(f"âœ… æ£€æµ‹å®Œæ¯•: Hidden Size = {self.hidden_size}")
        
        self.classifier = nn.Sequential(
            nn.Linear(self.hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 2)
        )

    def forward(self, s1, s2):
        outputs = self.backbone(s1, s2)
        if hasattr(outputs, 'last_hidden_state'):
            last_hidden = outputs.last_hidden_state
        elif isinstance(outputs, tuple):
            last_hidden = outputs[0]
        else:
            last_hidden = outputs
        return self.classifier(last_hidden[:, -1, :])

# ================= ğŸš€ ä¸»ç¨‹åº =================
def main():
    print(f"ğŸ§ª å¯åŠ¨å…¨é‡å¾®è°ƒå®éªŒ (Modulo Fix) | è®¾å¤‡: {DEVICE}")
    
    tokenizer = KronosTokenizer.from_pretrained(TOKENIZER_PATH)
    train_ds = SyntheticDataset(TRAIN_DATA, tokenizer)
    val_ds = SyntheticDataset(VAL_DATA, tokenizer)
    
    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
    val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)
    
    model = KronosClassifier(MODEL_PATH).to(DEVICE)
    
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()
    
    best_acc = 0.0
    
    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0
        train_acc = 0
        total_train = 0
        
        pbar = tqdm(train_dl, desc=f"Epoch {epoch+1} Train")
        
        for s1, s2, lbl in pbar:
            s1, s2, lbl = s1.to(DEVICE), s2.to(DEVICE), lbl.to(DEVICE)
            
            optimizer.zero_grad()
            logits = model(s1, s2)
            loss = criterion(logits, lbl)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            train_acc += (logits.argmax(1) == lbl).sum().item()
            total_train += lbl.size(0)
            
            pbar.set_postfix({'Loss': f"{loss.item():.4f}", 'Acc': f"{train_acc/total_train:.2%}"})
            
        model.eval()
        val_acc = 0
        total_val = 0
        with torch.no_grad():
            for s1, s2, lbl in tqdm(val_dl, desc=f"Epoch {epoch+1} Val"):
                s1, s2, lbl = s1.to(DEVICE), s2.to(DEVICE), lbl.to(DEVICE)
                logits = model(s1, s2)
                val_acc += (logits.argmax(1) == lbl).sum().item()
                total_val += lbl.size(0)
        
        tr_acc_pct = train_acc / total_train
        val_acc_pct = val_acc / total_val
        
        print(f"ğŸ“Š Summary: Train Acc: {tr_acc_pct:.2%} | Val Acc: {val_acc_pct:.2%}")
        
        if val_acc_pct > best_acc:
            best_acc = val_acc_pct
            torch.save(model.state_dict(), "best_full_finetune.pth")
            print("ğŸ’¾ æ–°çºªå½•ï¼å…¨é‡æ¨¡å‹å·²ä¿å­˜ã€‚")

if __name__ == "__main__":
    main()
