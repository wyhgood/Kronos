import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import os
import sys

# ================= âš™ï¸ æ ¸å¿ƒé…ç½®åŒºåŸŸ (åªæ”¹è¿™é‡Œ) =================

# 1. ä»»åŠ¡æ¨¡å¼
# "SCRATCH": ä»é›¶å¼€å§‹è®­ç»ƒ (ç”¨äºè·‘å¤§é‡åˆæˆæ•°æ®ï¼Œå¦‚åŒé¡¶å®éªŒ)
# "FINETUNE": åŠ è½½å·²æœ‰æ¨¡å‹å¾®è°ƒ (ç”¨äºè·‘çœŸå®æ•°æ®ï¼Œæˆ–è€…è¿›é˜¶åˆæˆæ•°æ®)
TRAIN_MODE = "SCRATCH" 

# 2. è·¯å¾„é…ç½®
DATA_DIR = "data_double_top_v1"   # æ•°æ®é›†æ–‡ä»¶å¤¹
MODEL_SAVE_NAME = "kronos_model.pth" # ä¿å­˜çš„æ¨¡å‹åå­—
PRETRAINED_PATH = "double_top_expert.pth" # å¦‚æœæ˜¯ FINETUNE æ¨¡å¼ï¼Œè¯»å–å“ªä¸ªæ¨¡å‹ï¼Ÿ

# 3. è®­ç»ƒè¶…å‚æ•°
BATCH_SIZE = 64
EPOCHS = 20
LEARNING_RATE = 1e-4
SEQ_LEN = 60
INPUT_DIM = 6   # OHLC + Vol + Amt
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"



# train_universal.py å¤´éƒ¨ä¿®æ”¹
# train_universal.py å¤´éƒ¨ä¿®æ”¹

# æŒ‡å‘æ–°æ•°æ®
DATA_DIR = "data_complex_ohlc_ema"
MODEL_SAVE_NAME = "kronos_ema_expert.pth"

# ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šè¾“å…¥ç»´åº¦ = 6 (OHLC + 2ä¸ªEMA)
INPUT_DIM = 6
# ================= ğŸ§  æ¨¡å‹å®šä¹‰ (æ ‡å‡†ç‰ˆ) =================
# è¿™æ˜¯ä¸€ä¸ªæ ‡å‡†çš„ Transformer æ¶æ„ï¼Œå…¼å®¹ä¸¤ç§æ¨¡å¼
class Kronos(nn.Module):
    def __init__(self, input_dim=6, d_model=128, nhead=4, num_layers=2):
        super().__init__()
        # æ•°å€¼åµŒå…¥å±‚ (Float -> Vector)
        self.embedding = nn.Linear(input_dim, d_model)
        
        # ä½ç½®ç¼–ç  (å­¦ä¹ å‹)
        self.pos_encoder = nn.Parameter(torch.zeros(1, SEQ_LEN, d_model))
        
        # Transformer ä¸»å¹²
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # åˆ†ç±»å¤´ (2åˆ†ç±»: ä¸è¿›/è¿›)
        self.fc = nn.Sequential(
            nn.Linear(d_model, 64),
            nn.ReLU(),
            nn.Linear(64, 2) 
        )

    def forward(self, x):
        # x: [Batch, Seq, Dim]
        x = self.embedding(x) + self.pos_encoder
        x = self.transformer(x)
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥ç‰¹å¾
        return self.fc(x[:, -1, :])

# ================= ğŸ’¾ æ•°æ®åŠ è½½å™¨ (é€šç”¨ç‰ˆ) =================
# ç»Ÿä¸€ä½¿ç”¨ .npy æ ¼å¼ï¼Œæ— è®ºæ˜¯çœŸå®æ•°æ®è¿˜æ˜¯åˆæˆæ•°æ®ï¼Œå…ˆè½¬æˆ npy å†å–‚è¿›æ¥
class UniversalDataset(Dataset):
    def __init__(self, x_path, y_path):
        if not os.path.exists(x_path):
            raise FileNotFoundError(f"âŒ æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶: {x_path}")
            
        self.X = np.load(x_path).astype(np.float32)
        self.y = np.load(y_path).astype(np.longlong) # Labelå¿…é¡»æ˜¯long
        
        # ğŸ”¥ è‡ªåŠ¨ Z-Score å½’ä¸€åŒ–
        print(f"ğŸ”„ æ­£åœ¨å½’ä¸€åŒ– {len(self.X)} æ¡æ•°æ®...")
        for i in range(len(self.X)):
            mean = np.mean(self.X[i], axis=0)
            std = np.std(self.X[i], axis=0) + 1e-6
            self.X[i] = (self.X[i] - mean) / std

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return torch.tensor(self.X[idx]), torch.tensor(self.y[idx])

# ================= ğŸš€ ä¸»ç¨‹åº =================
def main():
    print(f"ğŸš€ å¯åŠ¨é€šç”¨è®­ç»ƒå¼•æ“ | æ¨¡å¼: {TRAIN_MODE} | è®¾å¤‡: {DEVICE}")
    
    # 1. å‡†å¤‡æ•°æ®
    print(f"ğŸ“‚ è¯»å–æ•°æ®: {DATA_DIR}")
    try:
        train_ds = UniversalDataset(
            os.path.join(DATA_DIR, "X_train.npy"), 
            os.path.join(DATA_DIR, "y_train.npy")
        )
        # å¦‚æœæœ‰æµ‹è¯•é›†å°±åŠ è½½ï¼Œæ²¡æœ‰å°±è·³è¿‡
        test_path = os.path.join(DATA_DIR, "X_test.npy")
        if os.path.exists(test_path):
            test_ds = UniversalDataset(
                os.path.join(DATA_DIR, "X_test.npy"), 
                os.path.join(DATA_DIR, "y_test.npy")
            )
            test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)
        else:
            test_loader = None
            
        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return

    # 2. åˆå§‹åŒ–æ¨¡å‹
    model = Kronos(input_dim=INPUT_DIM).to(DEVICE)
    
    # 3. æƒé‡åŠ è½½é€»è¾‘ (æ ¸å¿ƒå·®å¼‚å¤„ç†)
    if TRAIN_MODE == "FINETUNE":
        if os.path.exists(PRETRAINED_PATH):
            print(f"ğŸ§  åŠ è½½é¢„è®­ç»ƒæƒé‡: {PRETRAINED_PATH}")
            model.load_state_dict(torch.load(PRETRAINED_PATH, map_location=DEVICE))
            
            # å¯é€‰ï¼šå¦‚æœæ˜¯å¾®è°ƒæå°‘é‡æ•°æ®ï¼Œå¯ä»¥å†»ç»“ä¸»å¹²
            # print("ğŸ”’ å†»ç»“ Transformer ä¸»å¹²...")
            # for param in model.transformer.parameters():
            #     param.requires_grad = False
        else:
            print(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°é¢„è®­ç»ƒæ¨¡å‹ {PRETRAINED_PATH}ï¼Œå°†ä»é›¶å¼€å§‹ï¼")
    else:
        print("âœ¨ åˆå§‹åŒ–å…¨æ–°æ¨¡å‹ (From Scratch)...")

    # 4. ä¼˜åŒ–å™¨ä¸æŸå¤±
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.CrossEntropyLoss()

    # 5. è®­ç»ƒå¾ªç¯
    print(f"\nğŸ å¼€å§‹è®­ç»ƒ | è½®æ•°: {EPOCHS}")
    best_acc = 0.0
    
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)
            
            optimizer.zero_grad()
            logits = model(X_batch)
            loss = criterion(logits, y_batch)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            preds = torch.argmax(logits, dim=1)
            correct += (preds == y_batch).sum().item()
            total += y_batch.size(0)
            
        train_acc = correct / total
        avg_loss = total_loss / len(train_loader)
        
        # æµ‹è¯•é›†éªŒè¯ (å¦‚æœæœ‰)
        test_log = ""
        if test_loader:
            model.eval()
            t_correct = 0
            t_total = 0
            with torch.no_grad():
                for X_t, y_t in test_loader:
                    X_t, y_t = X_t.to(DEVICE), y_t.to(DEVICE)
                    out = model(X_t)
                    t_correct += (torch.argmax(out, dim=1) == y_t).sum().item()
                    t_total += y_t.size(0)
            test_acc = t_correct / t_total
            test_log = f"| Val Acc: {test_acc:.2%}"
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if test_acc > best_acc:
                best_acc = test_acc
                torch.save(model.state_dict(), MODEL_SAVE_NAME)
                test_log += " â­"
        else:
            # å¦‚æœæ²¡æœ‰æµ‹è¯•é›†ï¼Œæ¯è½®éƒ½ä¿å­˜
            torch.save(model.state_dict(), MODEL_SAVE_NAME)

        print(f"Epoch {epoch+1:02d} | Loss: {avg_loss:.4f} | Train Acc: {train_acc:.2%} {test_log}")

    print(f"\nâœ… è®­ç»ƒç»“æŸã€‚æ¨¡å‹å·²ä¿å­˜ä¸º: {MODEL_SAVE_NAME}")

if __name__ == "__main__":
    main()
